{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math, random, csv, json, re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_video(td):\n",
    "    \"\"\"it's a video if it has exactly one pricelabel, and if\n",
    "    the stripped text inside that pricelabel starts with 'Video'\"\"\"\n",
    "    pricelabels = td('span', 'pricelabel')\n",
    "    return (len(pricelabels) == 1 and\n",
    "            pricelabels[0].text.strip().startswith(\"Video\"))\n",
    "\n",
    "def book_info(td):\n",
    "    \"\"\"given a BeautifulSoup <td> Tag representing a book,\n",
    "    extract the book's details and return a dict\"\"\"\n",
    "\n",
    "    title = td.find(\"div\", \"thumbheader\").a.text\n",
    "    by_author = td.find('div', 'AuthorName').text\n",
    "    authors = [x.strip() for x in re.sub(\"^By \", \"\", by_author).split(\",\")]\n",
    "    isbn_link = td.find(\"div\", \"thumbheader\").a.get(\"href\")\n",
    "    isbn = re.match(\"/product/(.*)\\.do\", isbn_link).groups()[0]\n",
    "    date = td.find(\"span\", \"directorydate\").text.strip()\n",
    "\n",
    "    return {\n",
    "        \"title\" : title,\n",
    "        \"authors\" : authors,\n",
    "        \"isbn\" : isbn,\n",
    "        \"date\" : date\n",
    "    }\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "def scrape(num_pages=31):\n",
    "    base_url = \"http://shop.oreilly.com/category/browse-subjects/\" + \\\n",
    "           \"data.do?sortby=publicationDate&page=\"\n",
    "\n",
    "    books = []\n",
    "\n",
    "    for page_num in range(1, num_pages + 1):\n",
    "        print(\"souping page\", page_num)\n",
    "        url = base_url + str(page_num)\n",
    "        soup = BeautifulSoup(requests.get(url).text, 'html5lib')\n",
    "\n",
    "        for td in soup('td', 'thumbtext'):\n",
    "            if not is_video(td):\n",
    "                books.append(book_info(td))\n",
    "\n",
    "        # now be a good citizen and respect the robots.txt!\n",
    "        sleep(30)\n",
    "\n",
    "    return books\n",
    "\n",
    "def get_year(book):\n",
    "    \"\"\"book[\"date\"] looks like 'November 2014' so we need to\n",
    "    split on the space and then take the second piece\"\"\"\n",
    "    return int(book[\"date\"].split()[1])\n",
    "\n",
    "def plot_years(plt, books):\n",
    "    # 2014 is the last complete year of data (when I ran this)\n",
    "    year_counts = Counter(get_year(book) for book in books\n",
    "                          if get_year(book) <= 2014)\n",
    "\n",
    "    years = sorted(year_counts)\n",
    "    book_counts = [year_counts[year] for year in x]\n",
    "    plt.bar([x - 0.5 for x in years], book_counts)\n",
    "    plt.xlabel(\"year\")\n",
    "    plt.ylabel(\"# of data books\")\n",
    "    plt.title(\"Data is Big!\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tab delimited stock prices:\n",
      "6/20/2014 AAPL 90.91\n",
      "6/20/2014 MSFT 41.68\n",
      "6/20/2014 FB 64.5\n",
      "6/19/2014 AAPL 91.86\n",
      "6/19/2014 MSFT 41.51\n",
      "6/19/2014 FB 64.34\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    def process(date, symbol, price):\n",
    "        print(date, symbol, price)\n",
    "\n",
    "    print(\"tab delimited stock prices:\")\n",
    "\n",
    "    with open('tab_delimited_stock_prices.txt', 'r', encoding='utf8',newline='') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        # reader = csv.reader(codecs.iterdecode(f, 'utf-8'), delimiter='\\t')\n",
    "        for row in reader:\n",
    "            date = row[0]\n",
    "            symbol = row[1]\n",
    "            closing_price = float(row[2])\n",
    "            process(date, symbol, closing_price)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colon delimited stock prices:\n",
      "6/20/2014 AAPL 90.91\n",
      "6/20/2014 MSFT 41.68\n",
      "6/20/2014 FB 64.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    print(\"colon delimited stock prices:\")\n",
    "\n",
    "    with open('colon_delimited_stock_prices.txt', 'r', encoding='utf8',newline='') as f:\n",
    "        reader = csv.DictReader(f, delimiter=':')\n",
    "        # reader = csv.DictReader(codecs.iterdecode(f, 'utf-8'), delimiter=':')\n",
    "        for row in reader:\n",
    "            date = row[\"date\"]\n",
    "            symbol = row[\"symbol\"]\n",
    "            closing_price = float(row[\"closing_price\"])\n",
    "            process(date, symbol, closing_price)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing out comma_delimited_stock_prices.txt\n",
      "BeautifulSoup\n",
      "<!DOCTYPE html>\n",
      "<html><head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\"/>\n",
      "    <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-type\"/>\n",
      "    <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>    \n",
      "</head>\n",
      "\n",
      "<body>\n",
      "<div>\n",
      "    <h1>Example Domain</h1>\n",
      "    <p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "\n",
      "\n",
      "</body></html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    print(\"writing out comma_delimited_stock_prices.txt\")\n",
    "\n",
    "    today_prices = { 'AAPL' : 90.91, 'MSFT' : 41.68, 'FB' : 64.5 }\n",
    "\n",
    "    with open('comma_delimited_stock_prices.txt','w', encoding='utf8',newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        for stock, price in today_prices.items():\n",
    "            writer.writerow([stock, price])\n",
    "\n",
    "    print(\"BeautifulSoup\")\n",
    "    html = requests.get(\"http://www.example.com\").text\n",
    "    soup = BeautifulSoup(html)\n",
    "    print(soup)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing json\n",
      "{'title': 'Data Science Book', 'author': 'Joel Grus', 'publicationYear': 2014, 'topics': ['data', 'science', 'data science']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    print(\"parsing json\")\n",
    "\n",
    "    serialized = \"\"\"{ \"title\" : \"Data Science Book\",\n",
    "                      \"author\" : \"Joel Grus\",\n",
    "                      \"publicationYear\" : 2014,\n",
    "                      \"topics\" : [ \"data\", \"science\", \"data science\"] }\"\"\"\n",
    "\n",
    "    # parse the JSON to create a Python object\n",
    "    deserialized = json.loads(serialized)\n",
    "    if \"data science\" in deserialized[\"topics\"]:\n",
    "        print(deserialized)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"https://api.github.com/users/jamespairepo/repos\"\n",
    "\n",
    "repos = json.loads(requests.get(endpoint).text)\n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "dates = [parse(repo[\"created_at\"]) for repo in repos]\n",
    "month_counts = Counter(date.month for date in dates)\n",
    "weekday_counts = Counter(date.weekday() for date in dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub API\n",
      "dates [datetime.datetime(2024, 3, 26, 3, 32, 33, tzinfo=tzutc()), datetime.datetime(2024, 3, 26, 3, 45, 59, tzinfo=tzutc()), datetime.datetime(2024, 3, 30, 23, 38, 44, tzinfo=tzutc()), datetime.datetime(2024, 3, 30, 7, 19, 10, tzinfo=tzutc()), datetime.datetime(2024, 3, 26, 3, 38, 14, tzinfo=tzutc()), datetime.datetime(2024, 3, 26, 3, 26, 51, tzinfo=tzutc()), datetime.datetime(2024, 3, 27, 22, 52, 49, tzinfo=tzutc()), datetime.datetime(2024, 3, 26, 3, 39, 26, tzinfo=tzutc()), datetime.datetime(2024, 3, 30, 3, 20, 53, tzinfo=tzutc()), datetime.datetime(2024, 3, 26, 4, 10, 45, tzinfo=tzutc()), datetime.datetime(2024, 3, 31, 6, 26, 29, tzinfo=tzutc())]\n",
      "month_counts Counter({3: 11})\n",
      "weekday_count Counter({1: 6, 5: 3, 2: 1, 6: 1})\n",
      "last five languages ['Jupyter Notebook', 'Jupyter Notebook', 'Jupyter Notebook', 'Jupyter Notebook', 'Jupyter Notebook']\n"
     ]
    }
   ],
   "source": [
    "    print(\"GitHub API\")\n",
    "    print(\"dates\", dates)\n",
    "    print(\"month_counts\", month_counts)\n",
    "    print(\"weekday_count\", weekday_counts)\n",
    "\n",
    "    last_5_repositories = sorted(repos,\n",
    "                                 key=lambda r: r[\"created_at\"],\n",
    "                                 reverse=True)[:5]\n",
    "\n",
    "    print(\"last five languages\", [repo[\"language\"]\n",
    "                                  for repo in last_5_repositories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import Twython\n",
    "\n",
    "# fill these in if you want to use the code\n",
    "CONSUMER_KEY = \"\"\n",
    "CONSUMER_SECRET = \"\"\n",
    "ACCESS_TOKEN = \"\"\n",
    "ACCESS_TOKEN_SECRET = \"\"\n",
    "\n",
    "def call_twitter_search_api():\n",
    "\n",
    "    twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "\n",
    "    # search for tweets containing the phrase \"data science\"\n",
    "    for status in twitter.search(q='\"data science\"')[\"statuses\"]:\n",
    "        user = status[\"user\"][\"screen_name\"].encode('utf-8')\n",
    "        text = status[\"text\"].encode('utf-8')\n",
    "        print(user, \":\", text)\n",
    "        print()\n",
    "\n",
    "from twython import TwythonStreamer\n",
    "\n",
    "# appending data to a global variable is pretty poor form\n",
    "# but it makes the example much simpler\n",
    "tweets = []\n",
    "\n",
    "class MyStreamer(TwythonStreamer):\n",
    "    \"\"\"our own subclass of TwythonStreamer that specifies\n",
    "    how to interact with the stream\"\"\"\n",
    "\n",
    "    def on_success(self, data):\n",
    "        \"\"\"what do we do when twitter sends us data?\n",
    "        here data will be a Python object representing a tweet\"\"\"\n",
    "\n",
    "        # only want to collect English-language tweets\n",
    "        if data['lang'] == 'en':\n",
    "            tweets.append(data)\n",
    "\n",
    "        # stop when we've collected enough\n",
    "        if len(tweets) >= 1000:\n",
    "            self.disconnect()\n",
    "\n",
    "    def on_error(self, status_code, data):\n",
    "        print(status_code, data)\n",
    "        self.disconnect()\n",
    "\n",
    "def call_twitter_streaming_api():\n",
    "    stream = MyStreamer(CONSUMER_KEY, CONSUMER_SECRET,\n",
    "                        ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "    # starts consuming public statuses that contain the keyword 'data'\n",
    "    stream.statuses.filter(track='data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
